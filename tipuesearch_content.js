var tipuesearch = {"pages":[{"title":"About Cody Hatch","text":"I am first and foremost a husband and father. I've worked in multiple capacities within the information security field since 2000 and currently work as a Sr. Sales Engineer at Exabeam , where I get to combine my passion for cyber security, machine learning, and data science. Email: comments@codyhatch.com Twitter: @codyhatch GitHub: @orangganjil","tags":"pages","url":"pages/about.html","loc":"pages/about.html"},{"title":"News and Podcast Episode","text":"Lately I've been pretty quiet due to a bunch of projects with which I've been involved. A big project has been an increase in the number of clients I've been working with in my business, Regression Labs, LLC . It's been busy and a challenge to keep up, but it's a great problem to have. The other thing I wanted to mention is that I was featured on Gina Colvin's A Thoughtful Faith podcast, on an episode titled \"A Cathedral Surrounded By Temples: Episcopalian in Utah: The Rev. Tyler Doherty and Cody Hatch\" . Just wanted to add an update to the site.","tags":"Blog","url":"blog/news-and-podcast-episode-2019-07.html","loc":"blog/news-and-podcast-episode-2019-07.html"},{"title":"US Mean Income","text":"There is a growing income disparity in the United States, which I find very worrisome. It doesn't seem sustainable and creates a nation of haves and have-nots. The graph below shows US mean household income, in 2017 CPI adjusted dollars, from 1967 to 2017. It is broken into quintiles and includes the top five percent of earners for comparison. I've also added the percentage growth for each line during the 50-year timespan. I also calculated the ratio of \"top five percent\"-to-\"lowest quintile\" for 1967 and 2017. 1967: 17.5/1 2017: 29.0/1","tags":"Data Analysis","url":"data-analysis/us-mean-income.html","loc":"data-analysis/us-mean-income.html"},{"title":"US Federal Income and Spending","text":"Part of being an informed citizen of the United States is to understand in what ways the federal government spends the money it collects, as well as the sources of government revenue. To aid in that understanding, I have provided three basic charts covering overall surplus/deficit, receipts, and expenses for the US federal government.","tags":"Data Analysis","url":"data-analysis/us-federal-income-spending.html","loc":"data-analysis/us-federal-income-spending.html"},{"title":"Analyzing LDS President Russell Nelson Talks About Family","text":"Several friends and I were recently discussing whether Wendy Watson Nelson has affected the topics about which LDS President Russell Nelson has chosen to speak, so I decided to perform a quick analysis of his General Conference talks going back to 1995. First, though, for those unfamiliar with why we would be interested in this question, I'll give a bit of background. According to LDS Newsroom , President Russell Nelson and Wendy Watson Nelson were married April 6, 2006. Wendy Watson was President Nelson's second wife. His first wife was Dantzel White Nelson, who died February 12, 2005. Sister Wendy Watson Nelson was a professor of marriage and family therapy at BYU prior to her marriage to President Nelson, and had written several books about relationships, marriage, and family. Since they have been married, she, along with President Nelson, has spoken at several firesides and face-to-face events, most recently in Las Vegas, Nevada. She frequently provides marriage and relationship advice at these events. She is highly educated, has teaching experience in these topics, is willing to speak frequently on these topics, and is much younger than President Nelson. Naturally, we were curious as to what type of influence, if any, she might have on President Nelson's talks. Since President and Sister Nelson were married in April 2006, in performing my analysis I had initially thought I'd grab all of President Nelson's talks going back to 1995 so as to get a similar number of years pre-marriage as there are available post-marriage; however, I was unable to find any way on lds.org to sort President Nelson's talks chronologically. I had considered sucking down all of his talks from the site and sorting them myself, but was reluctant to do so due to potential loads that might place on the Church's web servers (there are a lot of talks). It just seemed like I'd be crossing some line, so I settled on General Conference talks. I downloaded all of President Nelson's General Conference talks going back to April 1995 and ran them through a tool that performs a word count on the text. I kept the top ten words used in each talk and added them to a data frame within a Python program. Finally, I filtered the talks for words associated with marriage and family (the areas of Sister Nelson's expertise), and then graphed them on a time series chart along with a reference line for the Nelson's marriage.","tags":"Data Analysis","url":"data-analysis/analyzing-lds-president-russell-nelson-talks-family.html","loc":"data-analysis/analyzing-lds-president-russell-nelson-talks-family.html"},{"title":"Analyzing Questions for Oaks and Ballard Face-to-Face","text":"There is an upcoming event in the LDS Church (Mormon Church) where two leaders from the Church's hierarchy will answer questions submitted by some of the young single adults in the Church. There are about 2,600 questions submitted thus far and they are posted to a website. I wrote a Python script to scrape the questions and do text analysis on their content. I stemmed and tokenized the words, ran them through a count vectorizer to get term counts, and then performed non-negative matrix factorization to group the texts into topics. I then performed NMF on subsets of the questions which had been filtered according to various topics or keywords. I posted my analysis at Wheat & Tares: Analyzing Ballard and Oaks Face-to-Face Questions","tags":"Data Analysis","url":"data-analysis/analyzing-oaks-ballard-face-to-face-questions.html","loc":"data-analysis/analyzing-oaks-ballard-face-to-face-questions.html"},{"title":"Good Nginx HTTPS Configuration","text":"When configuring a web server for HTTPS, it is important to ensure your web server's configuration doesn't undermine the use of an encrypted session by using poor ciphers or not negotiating keys properly. Here are some Nginx options for a solid HTTPS/SSL configuration. I'm not going to cover obtaining SSL certificates and will just assume you have some. If not, head over to Let's Encrypt and obtain one for free. Once you've got your certificates figured out, within the Nginx configuration, find the appropriate server block and add the following: server { listen 80 ; listen 443 ssl ; root /var/www/site ; ssl_certificate /path/to/cert/chain.pem ; ssl_certificate_key /path/to/cert/privkey.pem ; ssl_protocols TLSv1.2 ; ssl_prefer_server_ciphers on ; ssl_ciphers ECDHE-RSA-AES256-GCM-SHA512 : DHE-RSA-AES256-GCM-SHA512 : ECDHE-RSA-AES256-GCM-SHA384 : DHE-RSA-AES256-GCM-SHA384 : ECDHE-RSA-AES256-SHA384 : ECDHE-RSA-AES256-SHA : DHE-RSA-AES256-SHA ; ssl_session_timeout 10m ; ssl_session_cache shared : SSL : 10 m ; ssl_session_tickets off ; ssl_stapling on ; ssl_stapling_verify on ; ... } The first portion of the configuration tells the server on which ports to listen, the web root directory, and the paths to your SSL certificates. I'll briefly describe the other lines. ssl_protocols Here we list the protocols the server is allowed to negotiate. In this case, we are only negotiating TLSv1.2 or above. ssl_prefer_server_ciphers This specifies that server ciphers should be preferred over client ciphers, which helps prevent a malicious client (typically via a man-in-the-middle attack) from downgrading to a less secure cipher than you'd prefer. Enabling this forces the client to accept only from among the server's offered ciphers. ssl_ciphers The list of ciphers that will be supported for the HTTPS session. We only want strong ciphers here so keep this up-to-date as cryptography marches on. ssl_session_timeout This specifies the time during which a client may reuse the session parameters. Don't set this for too long. ssl_session_cache Specifies the types and sizes of caches that can be used to store session parameters. Here we are using a cache shared among all worker processes on the server and limiting the cache to 10 megabytes in size. ssl_session_tickets Specifies whether session resumption through the use of tickets is enabled or disabled. If enabled, according to the RFC , the \"TLS server encapsulates the session state into a ticket and forwards it to the client. The client can subsequently resume a session using the obtained ticket.\" The configuration I have specified disables this functionality. ssl_stapling We are enabling the stapling of OCSP responses, which allows the server to send the certificate of the server's certificate issuer, saving client from having to make more roundtrips, which can be detrimental on a bandwidth constrained client. ssl_stapling_verify Enables the verification of OCSP responses by the server. The other thing you'll want to do is to redirect all HTTP requests to HTTPS sites. This can easily be done with redirects in Nginx, even if you need to redirect multiple domains (e.g., you own the .com, .net, and .org TLDs for a domain and want them all to go to one TLD). server { listen 80 ; listen 443 ssl ; ... server_name example.com www.example.com example.org www.example.org example.net www.example.net ; if ($host = 'www.example.com') { rewrite &#94;/(.*)$ https : // example . com / $ 1 permanent ; } if ($ host = 'http://example.com' ) { rewrite &#94;/(.*)$ https : // example . com / $ 1 permanent ; } if ($ host = 'example.org' ) { rewrite &#94;/(.*)$ https : // example . com / $ 1 permanent ; } if ($ host = 'www.example.org' ) { rewrite &#94;/(.*)$ https : // example . com / $ 1 permanent ; } if ($ host = 'example.net' ) { rewrite &#94;/(.*)$ https : // example . com / $ 1 permanent ; } if ($ host = 'www.example.net' ) { rewrite &#94;/(.*)$ https : // example . com / $ 1 permanent ; } ... } After all this is done, head over to the SSL Server Test run by Qualys SSL Labs. Enter your site and see what grade you receive.","tags":"Information Security","url":"information-security/nginx-good-https-configuration.html","loc":"information-security/nginx-good-https-configuration.html"},{"title":"Recent Updates","text":"I've given the site a major facelift, hopefully making it easier to get to some of my content. I've also removed my photography because I'm just not that interested in the topic any longer. Don't get me wrong, I still enjoy photography but talking about the topic and posting my photos just isn't as interesting to me as it used to be. Since I was updating the site, here are some additional updates on various projects with which I've been involved. First, I updated some of my public machine learning projects, such as my predictor of Lending Club defaults. They are available at my GitHub page . I've become a permanent blogger at the LDS blog Wheat & Tares . It's a great group of writers representing different types of Mormonism and I'm grateful they let me join their august ranks. You can find all of my posts at my author page . I'm still doing work for my small startup company, Regression Labs, LLC . I've got a few non-public projects I've been working on but hope I can release at least one of them soon.","tags":"Blog","url":"blog/recent-updates-2017-09.html","loc":"blog/recent-updates-2017-09.html"},{"title":"Lending Club Machine Learning Model and API","text":"This is an API that provides a probability of default when presented a loan from Lending Club. The API is written in Flask and it utilizes a scikit-learn machine learning model. You can get the code for this at the GitHub repo . Prerequisites Python 3 Flask Flask-RESTful numpy scipy pandas scikit-learn (0.19.0) - pickled model may be version dependent Installation Installation is completely up to you but it expects the typical Flask environment with the two pickled scikit-learn models in the same directory as the Flask app (this can be changed by modifying the joblib path within the app). Usage The API expects a JSON file conforming to the current format from Lending Club. There is an example JSON file called loanlist-example.json showing the format the model expects. There are three endpoints to the API: / (provides a readme/description) /version (provides a JSON output of the API version) /predict (this is the predictor) To use the API, POST a JSON file matching the format of the example file provided. Be sure to set your Content-Type to application\\json for the POST. The API will then load the loans into a Pandas DataFrame, make some modifications, and then run them through the Random Forest Classifier model. It then will add another column to the DataFrame for the predicted probability of default/charge-off. Finally, the API will then return JSON output of the memberId and probability of default for each loan that was input. Machine Learning Model The machine learning model is a Random Forest Classifier trained on all Lending Club loans from 2007 through the first half of 2017. It has the following parameters: RandomForestClassifier(bootstrap=True, class_weight='balanced', criterion='gini', max_depth=None, max_features='auto', max_leaf_nodes=None, min_impurity_split=1e-07, min_samples_leaf=5, min_samples_split=2, min_weight_fraction_leaf=0.0, n_estimators=100, n_jobs=3, oob_score=False, random_state=None, verbose=0, warm_start=False) Below is the confusion matrix followed by the accuracy, AUC, precision, and recall scores for the model: True Positive False Positive False Negative True Negative 74,319 4,546 1,870 16,393 Accuracy : 0.933942838316 Error : 0.0660571616836 AUC : 0.919982188297 Precision : 0.782893165863 Recall : 0.897607183924 If you are unfamiliar with a confusion matrix, in a binary decision such as whether an account is likely to default, the matrix lists in the first column the number of true positives over false positives. The second column lists the number of false negatives over true negatives. Ideally, one would like the numbers to be greatest going diagonally from top-left to bottom-right, with zeros in the other two cells. You can read more about a confusion matrix on Wikipedia . The two models used in the API are the following: Label Encoder Used for both the \"term\" and \"grade\" columns/features to encode them into labels. This, rather than one-hot encoding, was used due to the simplicity of the features and in an attempt to keep the feature size to a minimum (one of my goals was to enable this to run on a Raspberry Pi). It is important that this label encoder be loaded so that labels for new loans are consistent with those of the training set. Random Forest Classifier This model is a random forest classifier with 100 trees. It does the predicting, returning an array consisting of the probability a loan will not default along with the probability it will. The API is only returning the probability of default - the second field returned by the model. Performance The classifier is pretty quick and should work just fine on even a Raspberry Pi.","tags":"Projects","url":"projects/lending-club-default-predictor.html","loc":"projects/lending-club-default-predictor.html"},{"title":"Chlamydia Cases in Utah","text":"I've been exploring some communicable disease data for Utah (sourced from Utah's Open Data Portal which, unfortunately, only has data up through 2009) and created the chart below to illustrate something that completely surprised me: the most prevelant communicable disease in Utah over that ten-year period was Chlamydia, which was nearly nine times more prevalent than Chickenpox! To give you an idea of the difference, there were 43,832 cases of Chlamydia over that time period compared to 5,208 cases of Chickenpox. Here is a table showing the top five communicable diseases and, below that, a chart showing the growth of Chlamydia cases per year. Disease Total Cases Chlamydia 43,832 MRSA 10,434 Chicken Pox 5,208 Gonorrhea 5,057 Giardiasis 3,694","tags":"Data Analysis","url":"data-analysis/chlamydia-utah.html","loc":"data-analysis/chlamydia-utah.html"},{"title":"Utah Energy Production","text":"When I read so many articles about Utah's coal industry and the lack of real concern about coal-plant pollution in Utah, I reflect on this little chart I've had sitting around for some time now. I grabbed the data from Utah's Open Data Portal which, unfortunately, only has data up through 2013.","tags":"Data Analysis","url":"data-analysis/utah-energy-production.html","loc":"data-analysis/utah-energy-production.html"},{"title":"General Conference Word Frequency Estimator","text":"I recently updated a simple word frequency estimator for LDS General Conference talks that I have on GitHub. I had originally hacked it together but figured I'd update it to use the Python Natural Language Toolkit (NLTK) and a count vectorizor for word counts. You can find the code at the GitHub repo","tags":"Tools","url":"tools/genconf-word-frequency.html","loc":"tools/genconf-word-frequency.html"},{"title":"Writing Custom Splunk Apps","text":"I gave a presentation at BSidesSLC 2016 titled \"Writing Custom Splunk Apps\", where I reviewed the basics of writing a custom app for Splunk. Splunk is an incredibly useful data analysis and security tool, but getting the best out of it requires some organization and will inevitably require writing some custom apps to make use of that organization. This presentation was intended to provide some of the basics to get one started writing custom apps for Splunk. Check out the slide deck","tags":"Presentations","url":"presentations/writing-custom-splunk-apps-bsidesslc-2016-04.html","loc":"presentations/writing-custom-splunk-apps-bsidesslc-2016-04.html"},{"title":"DNS Registrar Checker","text":"I wrote a basic, hacked together Perl script to check domain registrar information and determine whether the DNS for your domain has been hijacked. I wrote the script a while ago in Perl and, since it works fine, I haven't bothered to change it. There are a few key things to note: You'll need to adjust some of the variables to fit your environment (e.g., domains, allowed IPs for DNS servers, etc.). I recommend that this run on a server or something where you can schedule it via cron to run at regular intervals. I recommend that you include administrative emails for notification that are not part of the domains this script monitors. If possible, run this from a server that can utilize split DNS to avoid any potential issues if the admin domains are hijacked. Again, this is a quickly hacked-together script but it works. Enjoy. The code is available at the GitHub repo .","tags":"Tools","url":"tools/dns-registrar-checker.html","loc":"tools/dns-registrar-checker.html"},{"title":"Application Security Preso at Hack Ogden","text":"Here are the slides of the presentation I gave at Hack Ogden today. It was a great crowd with great questions. It's always great to chat about security with those who are outside the infosec echo chamber, and the folks at Hack Ogden were a wonderful bunch with whom to chat. Nerding Out on Application Security","tags":"Presentations","url":"presentations/application-security-hack-ogden-03-2015.html","loc":"presentations/application-security-hack-ogden-03-2015.html"},{"title":"Configure Nginx to Protect Ghost Login Page","text":"In information security, it is not possible to be correct all the time - things will break, controls will fail, and software bugs will exist; those things are inevitable, so a key element of information security is to reduce the attack surface of a system. Wikipedia has a good definition of \"attack surface\": \"The attack surface of a software environment is the code within a computer system that can be run by unauthorized users. This includes, but is not limited to: user input fields, protocols, interfaces, and services. OSSTMM 3 Defines Attack Surface as 'The lack of specific separations and functional controls that exist for that vector'. One approach to improving information security is to reduce the attack surface of a system or software. By turning off unnecessary functionality, there are fewer security risks. By having less code available to unauthorized actors, there will tend to be fewer failures. Although attack surface reduction helps prevent security failures, it does not mitigate the amount of damage an attacker could inflict once a vulnerability is found.\" Ghost is some fantastic software but will inevitably have security vulnerabilities, especially in the complex pieces within the administration functions. It would be a good idea to reduce Ghost's attack surface by restricting what systems can even access the administrative portion of Ghost, and access controls within Nginx allow us to do so if we are using Nginx to proxy connections to Ghost. Access controls within Nginix are pretty straightforward. For example, if you want to allow a specific IP address to access a portion of your site, but deny all others, you'd define the location and access controls within the server block: server { ... location /mysecretlocation/ { allow 1.1.1.1; deny all; } ... } That's pretty straightforward, causing Nginx to allow connections to www.example.com/mysecretlocation by the IP address 1.1.1.1, while kicking a 403 Forbidden message to all others. This gets a little different when using Nginx to proxy connections to Ghost, but it is still pretty straightforward - we just need to add the proxy information to the location block, like this: server { ... location /ghost/ { allow 1.1.1.1; deny all; proxy_set_header X-Real-IP $remote_addr; proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for; proxy_set_header X-Forwarded-Proto $scheme; proxy_set_header Host $http_host; proxy_set_header Connection \"\"; proxy_http_version 1.1; proxy_pass http://ghost_upstream_defined_in_nginx_conf; } ... } This config does the same thing as before but applies it to the proxy for Ghost, which is important since the URI of /ghost/signin doesn't actually exist in the website's directory root, but is only understood by Ghost. However, if an IP other than 1.1.1.1 were to go to www.mywebsite.com/ghost/signin, they would get a 403 Forbidden message. Now, when a vulnerability is identified that affects Ghost's administration functionality, the website's exposure to that vulnerability should be greatly reduced. If you have any feedback or questions, contact me on Twitter at @codyhatch until I get my contact page up on this site.","tags":"Information Security","url":"information-security/nginx-protect-ghost-login.html","loc":"information-security/nginx-protect-ghost-login.html"},{"title":"Nginx Tip - Remove Version Number","text":"I wanted to give another quick tip in the process of hardening the security of Nginx. There are a bunch of things one can do to manage and limit connections, and they're good, but something that is often overlooked is removing the version number of the web server. This isn't a security-through-obscurity thing, but rather taking whatever easy steps one can take to make things even a little bit tougher for an attacker. For example, if there is a vulnerability in a specific version of Nginx, it makes sense not to broadcast the version number running on one's server to the world, allowing anyone paying attention to easily determine whether your server is vulnerable. Of course, one can go to great lengths and change the server software name altogether by modifying Nginx's source code and recompiling, but I'm going to limit this suggestion to something that is easy and within the reach of every web admin out there. To configure Nginx to stop listing its version number in error pages and HTTP headers, add the following line to the server block in your configuration file: server_tokens : off ; That's all that needs to be done. Now, Nginx will only list the fact that it is an Nginx server, but exclude the version in HTTP headers and error pages.","tags":"Information Security","url":"information-security/nginx-remove-version-number.html","loc":"information-security/nginx-remove-version-number.html"}]};